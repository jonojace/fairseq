{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105eb05b-33bc-41e5-9407-334da3bf3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21931792-66f9-41ca-a3fa-3bce6c878701",
   "metadata": {},
   "source": [
    "# get speech reps for all utts in lj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086b3094-f001-4356-af37-8be6887d604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"/home/s1785140/fairseq/examples/speech_audio_corrector/lj_speech_quantized.txt\"\n",
    "\n",
    "# load file contents\n",
    "with open(fp, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# return dict mapping from id to speech rep codes\n",
    "\n",
    "ids2speechreps = {}\n",
    "\n",
    "for l in lines:\n",
    "    utt_id, codes = l.split('|')\n",
    "    codes = codes.rstrip() # strip trailing newline char\n",
    "    codes = [int(s) for s in codes.split(' ')] # convert from str of ints to list of ints\n",
    "    ids2speechreps[utt_id] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9028bdf0-0787-4b03-9dbe-cf1defcb530a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids2speechreps['LJ004-0001'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d241475-ca40-4faa-8c83-745886c5ce2a",
   "metadata": {},
   "source": [
    "# get all word alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8462e108-444c-42d7-be23-48348593817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textgrid\n",
    "from collections import Counter\n",
    "\n",
    "def get_word_alignments(\n",
    "        textgrid_path,\n",
    "        utt_dur_from_last_word=False,\n",
    "        ignore_list=['<unk>'],\n",
    "):\n",
    "    \"\"\"\n",
    "    extract word alignments from textgrid file corresponding to one utterance\n",
    "    \n",
    "    utt_dur_from_last_word: whether to set utt_dur to end timestamp of  last real wordtype, or from \n",
    "    the very last alignment in the utterance (likely corresponding to silence)\n",
    "    \"\"\"\n",
    "    tg = textgrid.TextGrid.fromFile(textgrid_path)\n",
    "    words_intervaltier, _phones_intervaltier = tg\n",
    "    words = []\n",
    "    counter = Counter()\n",
    "\n",
    "    for word in words_intervaltier:        \n",
    "        if word.mark and word.mark not in ignore_list: # if word.mark is False then it is SILENCE\n",
    "            counter[word.mark] += 1\n",
    "            words.append({\n",
    "                \"wordtype\": word.mark,\n",
    "                \"utt_id\": textgrid_path.split('/')[-1].split('.')[0],\n",
    "                \"example_no\": counter[word.mark], # the number of times we have seen this word in this utterance\n",
    "                \"start\": word.minTime,\n",
    "                \"end\": word.maxTime,\n",
    "            })\n",
    "            \n",
    "    if utt_dur_from_last_word:\n",
    "        # use last real word end time as the utt_dur\n",
    "        utt_dur = words[-1]['end']\n",
    "    else:\n",
    "        # at this point word is the last item in words_intervaltier (most likely sil / None)\n",
    "        utt_dur = word.maxTime\n",
    "            \n",
    "    # add utt_dur to all words\n",
    "    for w in words:\n",
    "        w[\"utt_dur\"] = utt_dur\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4cf3b1-83f5-4cd9-a1d0-b3efe2151d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count is 0\n"
     ]
    }
   ],
   "source": [
    "alignment_dir = \"/home/s1785140/data/ljspeech_MFA_alignments_from_fb\"\n",
    "\n",
    "count = 0\n",
    "\n",
    "MAX_TO_PROCESS = 100\n",
    "ids = list(ids2speechreps.keys())[:MAX_TO_PROCESS]\n",
    "\n",
    "ids2word_alignments = {}\n",
    "\n",
    "for utt_id in ids:\n",
    "    words_align = get_word_alignments(textgrid_path=f\"{alignment_dir}/{utt_id}.TextGrid\", utt_dur_from_last_word=False)\n",
    "    ids2word_alignments[utt_id] = words_align\n",
    "    \n",
    "    \n",
    "    # check for words that contain non alphabet chars such as <unk> token\n",
    "    # print(words_align)\n",
    "    \n",
    "    for w in words_align:\n",
    "        if '<' in w['wordtype']:\n",
    "            # print(w, words_align)\n",
    "            if w['wordtype'] != '<unk>':\n",
    "                print(\"not <unk>:\", w['wordtype'])\n",
    "            count += 1\n",
    "            \n",
    "        if not w['wordtype']:\n",
    "            print('word is FALSE', w, words_align)\n",
    "\n",
    "print(\"count is\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad61c210-c377-46d6-b198-e0b1313aaa18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids2word_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bed508-8a3d-4b64-b3de-985730e34396",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# create wordtype to speech aligned feats data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44898e04-ed42-4468-9589-9a621840744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordlevel_reprs(speechreps, word_align):\n",
    "    \"\"\"\n",
    "    extract subsequence of 'repr' that corresponds to a particular word\n",
    "    function expects input to be of dimension 2: (timesteps, hidden_size)\n",
    "    \"\"\"\n",
    "    start_fraction = word_align['start'] / word_align['utt_dur']\n",
    "    end_fraction = word_align['end'] / word_align['utt_dur']\n",
    "    timesteps = len(speechreps)\n",
    "    start_idx = round(start_fraction * timesteps)\n",
    "    end_idx = round(end_fraction * timesteps)\n",
    "    return speechreps[start_idx:end_idx]\n",
    "\n",
    "word2speechreps = {}\n",
    "\n",
    "for utt_id in ids:\n",
    "    speech_reps = ids2speechreps[utt_id]\n",
    "    word_aligns = ids2word_alignments[utt_id]\n",
    "    \n",
    "    for word_align in word_aligns:\n",
    "        word_align['speech_reps'] = get_wordlevel_reprs(speech_reps, word_align)\n",
    "        \n",
    "        # following info to debug whether alignments are consistent in len\n",
    "        # word_align['speech_reps_len'] = len(word_align['speech_reps'])\n",
    "        # word_align['speech_reps_len_dur_ratio'] = word_align['speech_reps_len'] / (word_align['end']-word_align['start'])\n",
    "        \n",
    "        wordtype = word_align['wordtype']\n",
    "        example_no = word_align['example_no']\n",
    "        unique_id = utt_id + '|' + str(example_no)\n",
    "        \n",
    "        if wordtype not in word2speechreps:\n",
    "            word2speechreps[wordtype] = {}\n",
    "        word2speechreps[wordtype][unique_id] = word_align['speech_reps']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3fb86-c4d1-4b3c-ae78-5cbccf9c3373",
   "metadata": {},
   "source": [
    "# implement fn to get position of each word in the text seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd293c2f-202a-4983-94f8-1846aaf7f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfa_text(word_align):\n",
    "    return \" \".join(w['wordtype'] for w in word_align)\n",
    "\n",
    "def get_mfa_text_from_utt_id(utt_id):\n",
    "    word_align = ids2word_alignments[utt_id]\n",
    "    return get_mfa_text(word_align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7691cfd-b84d-4044-b0aa-28fdef9b335b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntervalTier(words, [Interval(0.0, 0.42, captain), Interval(0.42, 1.06, williams), Interval(1.06, 1.16, who), Interval(1.16, 1.41, was), Interval(1.41, 1.54, the), Interval(1.54, 2.16, inspector), Interval(2.16, 2.27, of), Interval(2.27, 2.75, prisons), Interval(2.75, 2.89, for), Interval(2.89, 2.97, the), Interval(2.97, 3.27, home), Interval(3.27, 3.83, district), Interval(3.83, 3.86, None), Interval(3.86, 4.01, in), Interval(4.01, 4.65, succession), Interval(4.65, 4.84, to), Interval(4.84, 5.28, messrs), Interval(5.28, 5.86, crawford), Interval(5.86, 5.89, None), Interval(5.89, 6.09, and), Interval(6.09, 6.6, russell), Interval(6.6, 6.65964, None)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg = textgrid.TextGrid.fromFile(f\"{alignment_dir}/{utt_id}.TextGrid\")\n",
    "words_intervaltier, _phones_intervaltier = tg\n",
    "words_intervaltier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c4bb3e7-ad15-406f-b164-50be8ee23aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfa_text = get_mfa_text(word_aligns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e372161d-8373-4370-b530-9b9a671f3fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'captain williams who was the inspector of prisons for the home district in succession to messrs crawford and russell'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfa_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb1aed7f-10be-4d3d-addb-49a8f483fb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ h o w _ a r e _ y o u\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('how', 1), ('are', 2), ('you', 3)], [0, 1, 1, 1, 0, 2, 2, 2, 0, 3, 3, 3, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_pos_2(text, whitespace_tok=\"_\", boundary_same_pos=True, with_eos=True, boundary_pos=0):\n",
    "    \"\"\"\n",
    "    return words and their word pos\n",
    "    \n",
    "    and also word pos of each grapheme in the seq\n",
    "    \"\"\"\n",
    "    graphemes = text.split(' ')\n",
    "    \n",
    "    # double check that we are dealing with a seq output by bpe tokenizer\n",
    "    assert graphemes[0] == whitespace_tok \n",
    "    \n",
    "    word_count = 0\n",
    "    word_and_word_pos = []\n",
    "    word_pos_of_graphemes = []\n",
    "    current_word = \"\"\n",
    "    \n",
    "    for i, c in enumerate(graphemes):\n",
    "        # reached the last char of the utt\n",
    "        if i == len(graphemes) - 1: \n",
    "            current_word += c # add last char\n",
    "            word_and_word_pos.append((current_word, word_count)) # add last word\n",
    "            word_pos_of_graphemes.append(word_count)\n",
    "            \n",
    "        # whitespace\n",
    "        elif c == whitespace_tok: \n",
    "            if current_word: # at a whitespace token AFTER processing at least one word\n",
    "                word_and_word_pos.append((current_word, word_count))\n",
    "                current_word = \"\"\n",
    "            if boundary_same_pos:\n",
    "                word_pos_of_graphemes.append(boundary_pos)\n",
    "            else:\n",
    "                word_count += 1 # because we count each whitespace_tok as a new word position\n",
    "                word_pos_of_graphemes.append(word_count)\n",
    "                \n",
    "        # processing a grapheme in a word\n",
    "        else: \n",
    "            if graphemes[i-1] == whitespace_tok:\n",
    "                word_count += 1 # only increment word position if we are at the beginning of a new word, not within it\n",
    "            word_pos_of_graphemes.append(word_count)\n",
    "            current_word += c\n",
    "            \n",
    "    if with_eos:\n",
    "        word_pos_of_graphemes.append(word_count+1) \n",
    "            \n",
    "    return word_and_word_pos, word_pos_of_graphemes\n",
    "    \n",
    "words = [\"how\" , \"are\", \"you\"]\n",
    "txt = \"_\"+ \"_\".join(words)\n",
    "txt = \" \".join([c for c in txt])\n",
    "print(txt)\n",
    "get_word_pos_2(txt, whitespace_tok=\"_\", boundary_same_pos=True, with_eos=True, boundary_pos=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d85ba1b-84f6-497d-bd90-5ffce9b2884d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_ h o w _ a r e _ y o u'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create some input text for testing\n",
    "words = [\"how\" , \"are\", \"you\"]\n",
    "txt = \"_\"+ \"_\".join(words)\n",
    "txt = \" \".join([c for c in txt])\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18e9faf6-5325-433f-8950-fde87ff3aa62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('how', 1), ('are', 2), ('you', 3)], [0, 1, 1, 1, 0, 2, 2, 2, 0, 3, 3, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_pos_2(txt, boundary_same_pos=True, with_eos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3abcf4e6-2ab7-4975-9d47-5a5df5d5b057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('how', 1), ('are', 2), ('you', 3)], [0, 1, 1, 1, 0, 2, 2, 2, 0, 3, 3, 3, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_pos_2(txt, boundary_same_pos=True, with_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6902cdbb-6092-4ed5-87f9-752738ff4ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('how', 2), ('are', 4), ('you', 6)], [1, 2, 2, 2, 3, 4, 4, 4, 5, 6, 6, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_pos_2(txt, boundary_same_pos=False, with_eos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f05b3083-3668-49ab-b1e0-7e2302e28371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('how', 2), ('are', 4), ('you', 6)], [1, 2, 2, 2, 3, 4, 4, 4, 5, 6, 6, 6, 7])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_pos_2(txt, boundary_same_pos=False, with_eos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d049aa-c972-430e-8d44-c71b5e9dac8e",
   "metadata": {},
   "source": [
    "# implement getting speech reps for words in an utterance \n",
    "\n",
    "Also add ability for regularisation:\n",
    "    * shuffling word examples\n",
    "    * removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acf21b20-d316-48ec-971d-434203d7f585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 5), (3, 5)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_len_encoding(seq):\n",
    "    \"\"\"encode a seq using run length encoding\n",
    "    \n",
    "    e.g. [1,2,2,2,2,2,3,3,3,3,3] -> [(1, 1), (2, 5), (3, 5)]\n",
    "    \"\"\"\n",
    "    encoding = []\n",
    "    prev_char = ''\n",
    "    count = 1\n",
    "\n",
    "    if not seq: return []\n",
    "\n",
    "    for char in seq:\n",
    "        # If the prev and current characters\n",
    "        # don't match...\n",
    "        if char != prev_char:\n",
    "            # ...then add the count and character\n",
    "            # to our encoding\n",
    "            if prev_char:\n",
    "                encoding.append((prev_char, count))\n",
    "            count = 1\n",
    "            prev_char = char\n",
    "        else:\n",
    "            # Or increment our counter\n",
    "            # if the characters do match\n",
    "            count += 1\n",
    "    else:\n",
    "        # Finish off the encoding\n",
    "        encoding.append((prev_char, count))\n",
    "        return encoding\n",
    "        \n",
    "\n",
    "speechreps = [1,2,2,2,2,2,3,3,3,3,3]\n",
    "run_len_encoding(speechreps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8554d913-5479-4129-b87f-99738cee18f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 1), (3, 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_dups_random(rle, min_count=1):\n",
    "    \"\"\"return a rle where each char's count is reduced a random amount\"\"\"\n",
    "    compressed_rle = []\n",
    "    for char, count in rle:\n",
    "        new_count = random.randint(min_count, count)\n",
    "        compressed_rle.append((char, new_count))\n",
    "    return compressed_rle\n",
    "\n",
    "speechreps = [1,2,2,2,2,2,3,3,3,3,3]\n",
    "rle = run_len_encoding(speechreps)\n",
    "remove_dups_random(rle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c95fd88b-f902-4d07-b328-4c21aac373bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressed [(1, 1), (2, 5), (3, 5)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_rle(rle):\n",
    "    \"\"\"expand an RLE back to a list\"\"\"\n",
    "    expanded_rle = []\n",
    "    for char, count in rle:\n",
    "        expanded_rle.extend(count*[char])\n",
    "    return expanded_rle\n",
    "\n",
    "speechreps = [1,2,2,2,2,2,3,3,3,3,3]\n",
    "rle = run_len_encoding(speechreps)\n",
    "print(\"compressed\", rle)\n",
    "expand_rle(rle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0018daaa-97f4-4105-880c-071992034a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "def collapse_dups(speechreps, remove_dup_prob, remove_dup_rand_num):\n",
    "    \"\"\"take a list of elements and remove duplicates\n",
    "    \n",
    "    optionally do not remove all duplicates but remove a random amount\n",
    "    \n",
    "    TODO add option of sometimes ADDING codes? to make neural model more robust to duration changes\n",
    "    \"\"\"\n",
    "    if remove_dup_prob > 0.0 and random.random() > (1.0 - remove_dup_prob):\n",
    "        rle = run_len_encoding(speechreps)\n",
    "        if remove_dup_rand_num:\n",
    "            compressed_rle = remove_dups_random(rle)\n",
    "        else:\n",
    "            # remove all duplicates for each code (i.e. set count to 0)\n",
    "            compressed_rle = [(char, 1) for char, count in rle]\n",
    "        speechreps = expand_rle(compressed_rle)\n",
    "    return speechreps\n",
    "\n",
    "speechreps = [1,1,1,1,1,2,2,2,2,2,3,3,3,3,3]\n",
    "print(\"original\", speechreps)\n",
    "for _ in range(10):\n",
    "    collapse_dups(speechreps, remove_dup_prob=0.5, remove_dup_rand_num=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "470d62e0-cb16-48cc-8841-c183c2c4daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_timesteps(seq, p):\n",
    "    \"\"\"randomly dropout timesteps seq\"\"\"\n",
    "    if p > 0.0 :\n",
    "        new_seq = []\n",
    "        for c in seq:\n",
    "            if random.random() < (1.0 - p):\n",
    "                new_seq.append(c)\n",
    "            else:\n",
    "                pass\n",
    "        return new_seq\n",
    "    else:\n",
    "        return seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff85f133-e249-442e-9585-44f8f3939105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[82, 82, 73, 73, 70]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_speechreps_for_word(word, utt_id, count_of_word, word2speechreps, randomise, \n",
    "                            remove_dup_prob, remove_dup_rand_num, dropout_p):\n",
    "    \"\"\"return the speechreps for a wordtype\n",
    "    \n",
    "    optionally remove duplicates\"\"\"\n",
    "    unique_id = f\"{utt_id}|{count_of_word}\"\n",
    "    \n",
    "    # get speechreps corresponding to word\n",
    "    if not randomise and unique_id in word2speechreps[word]:\n",
    "        word_reps = word2speechreps[word][unique_id]\n",
    "    else:\n",
    "        random_unique_id = random.sample(word2speechreps[word].keys(), k=1)[0]\n",
    "        word_reps = word2speechreps[word][random_unique_id]\n",
    "    \n",
    "    # optionally collapse duplicate codes\n",
    "    word_reps = collapse_dups(word_reps, remove_dup_prob=remove_dup_prob, remove_dup_rand_num=remove_dup_rand_num)\n",
    "    \n",
    "    # optionally randomly dropout codes\n",
    "    word_reps = dropout_timesteps(word_reps, p=dropout_p)\n",
    "        \n",
    "    return word_reps\n",
    "\n",
    "word = \"the\"\n",
    "utt_id = \"LJ033-0206\"\n",
    "count_of_word = 1\n",
    "# unique_id = \"LJ033-0206\" + \"|\" + \"1\"\n",
    "get_speechreps_for_word(word, utt_id, count_of_word, word2speechreps, randomise=False, \n",
    "                        remove_dup_prob=0.0, remove_dup_rand_num=False, dropout_p=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63cf6c3a-ac14-4c10-915d-b0812ab34e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confirmed that the rifle could have picked up fibers from the blanket and transferred them to the paper bag\n",
      "_ c o n f i r m e d _ t h a t _ t h e _ r i f l e _ c o u l d _ h a v e _ p i c k e d _ u p _ f i b e r s _ f r o m _ t h e _ b l a n k e t _ a n d _ t r a n s f e r r e d _ t h e m _ t o _ t h e _ p a p e r _ b a g\n",
      "[('confirmed', 1), ('that', 2), ('the', 3), ('rifle', 4), ('could', 5), ('have', 6), ('picked', 7), ('up', 8), ('fibers', 9), ('from', 10), ('the', 11), ('blanket', 12), ('and', 13), ('transferred', 14), ('them', 15), ('to', 16), ('the', 17), ('paper', 18), ('bag', 19)]\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 0, 3, 3, 3, 0, 4, 4, 4, 4, 4, 0, 5, 5, 5, 5, 5, 0, 6, 6, 6, 6, 0, 7, 7, 7, 7, 7, 7, 0, 8, 8, 0, 9, 9, 9, 9, 9, 9, 0, 10, 10, 10, 10, 0, 11, 11, 11, 0, 12, 12, 12, 12, 12, 12, 12, 0, 13, 13, 13, 0, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 15, 15, 15, 15, 0, 16, 16, 0, 17, 17, 17, 0, 18, 18, 18, 18, 18, 0, 19, 19, 19]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]\n"
     ]
    }
   ],
   "source": [
    "def get_speechreps_for_utt(word_and_word_pos, utt_id, word2speechreps, \n",
    "                           randomise_examples=False, remove_dup_prob=0.0, \n",
    "                           remove_dup_rand_num=False, dropout_p=0.0):\n",
    "    \"\"\"\n",
    "    get speech reps for all the words in an utterance\n",
    "    \n",
    "    optionally:\n",
    "        - randomly retrieve speech reps for different examples of the word\n",
    "        - remove duplicate codes\n",
    "        - dropout codes\n",
    "    \"\"\"\n",
    "    speechreps, speechreps_word_pos, word_counter = [], [], Counter()\n",
    "    \n",
    "    for word, word_pos in word_and_word_pos:\n",
    "        word_counter[word] += 1\n",
    "        word_speechreps = get_speechreps_for_word(word, utt_id, word_counter[word], word2speechreps, randomise=randomise_examples, \n",
    "                                                  remove_dup_prob=remove_dup_prob, remove_dup_rand_num=remove_dup_rand_num, \n",
    "                                                  dropout_p=dropout_p)\n",
    "        speechreps.extend(word_speechreps)\n",
    "        speechreps_word_pos.extend(len(word_speechreps)*[word_pos])\n",
    "        \n",
    "        # TODO add interword separator tokens \n",
    "        # TODO <sep> or \"_\" according to tgt_dict\n",
    "        \n",
    "    return speechreps, speechreps_word_pos\n",
    "        \n",
    "                \n",
    "utt_id = \"LJ033-0206\"\n",
    "mfa_text = get_mfa_text_from_utt_id(utt_id)\n",
    "print(mfa_text)\n",
    "mfa_text = mfa_text.split(\" \")\n",
    "mfa_text = \"_\"+ \"_\".join(mfa_text)\n",
    "mfa_text = \" \".join([c for c in mfa_text])\n",
    "print(mfa_text)\n",
    "word_and_word_pos, word_pos_of_graphemes = get_word_pos_2(mfa_text, boundary_same_pos=True, with_eos=False)\n",
    "print(word_and_word_pos)\n",
    "print(word_pos_of_graphemes)\n",
    "speechreps, speechreps_word_pos = get_speechreps_for_utt(word_and_word_pos, utt_id, word2speechreps, \n",
    "                           randomise_examples=False, remove_dup_prob=1.0, remove_dup_rand_num=True)\n",
    "\n",
    "print(speechreps_word_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e8e90-5ec0-4a84-9637-1348e8a9a2b2",
   "metadata": {},
   "source": [
    "# helpers for dictionary encoding of speech reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4afe3924-fcba-4326-8570-095c2a9d4b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HUB1 HUB2 HUB3 HUB2 HUB3 HUB2 HUB2 HUB2 HUB2 HUB1 HUB2 HUB3'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prep_speechreps_for_dict_encoding(speechreps):\n",
    "    \"\"\"\n",
    "    take hubert codes (int from 0 to K-1 where K is number of k-means clusters)\n",
    "    return a string version suitable for dictionary encoding\n",
    "    \"\"\"\n",
    "    new_speechreps = []\n",
    "    for x in speechreps:\n",
    "        new_speechreps.append(f\"HUB{x}\")\n",
    "    return \" \".join(new_speechreps)\n",
    "    \n",
    "prep_speechreps_for_dict_encoding([1,2,3,2,3,2,2,2,2,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0ea1f-7a98-4e49-8dd5-66f9c245310a",
   "metadata": {},
   "source": [
    "# helpers for generating word masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6900817-f141-493a-842b-e5de81795d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1, 2}, {3, 4, 5, 6, 7, 8, 9, 10})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def two_random_partitions(indices, p=0.5):\n",
    "    \"\"\"given a list of indices (indicating word positions)\n",
    "    partition into two sets\n",
    "    p is probability of entering set1\n",
    "    \"\"\"\n",
    "    set1, set2 = set(), set()\n",
    "    for idx in indices:\n",
    "        if random.random() > (1.0 - p):\n",
    "            set1.add(idx)\n",
    "        else:\n",
    "            set2.add(idx)\n",
    "    return set1, set2\n",
    "\n",
    "two_random_partitions(list(range(1,11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "58be4810-ac7a-4078-b22b-79c2ac039d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pos(graphemes, padding_idx, bpe_whitespace_tok=\"▁\", boundary_same_pos=True,\n",
    "                 append_eos=False, eos_symbol = \"</s>\", boundary_start_pos=None):\n",
    "    \"\"\"\n",
    "    for some space delimited sequence of symbols (e.g. text)\n",
    "\n",
    "    return words and their word pos\n",
    "\n",
    "    and also word pos of each grapheme in the seq (a list of the same length,\n",
    "    of ints representing the words that each symbol / whitespace corresponds to)\n",
    "    \n",
    "    by default the boundary start position is initiated as padding_idx + 1\n",
    "    and then word counts start from that value\n",
    "\n",
    "    args:\n",
    "        text: str of space delimited graphemes in the utterance ('_' denotes whitespace in the original utterance)\n",
    "              e.g. \"_ h o w _ a r e _ y o u\" this is the format returned by sentence piece tokeniser\n",
    "\n",
    "    e.g.\n",
    "    _ h o w _ a r e _ y o u\n",
    "        padding_idx == 1\n",
    "        boundary_start_pos == 2\n",
    "        boundary_same_pos == True\n",
    "        \n",
    "        before padding:\n",
    "            [('how', 3), ('are', 4), ('you', 5)]\n",
    "            [2, 3, 3, 3, 2, 4, 4, 4, 2, 5, 5, 5, 6]\n",
    "        after concat with speechreps and padding (not performed in this fn, performed in SAC dataset collater):\n",
    "            [2, 3, 3, 3, 2, 4, 4, 4, 2, 5, 5, 5, 6, <speechreps>, 1, 1, 1, ...]\n",
    "            \n",
    "    _ h o w _ a r e _ y o u\n",
    "        padding_idx == 1\n",
    "        boundary_start_pos == 2\n",
    "        boundary_same_pos == False\n",
    "        \n",
    "        before padding:\n",
    "            [('how', 3), ('are', 5), ('you', 7)]\n",
    "            [2, 3, 3, 3, 4, 5, 5, 5, 6, 7, 7, 7, 8]\n",
    "        after concat with speechreps and padding (not performed in this fn, performed in SAC dataset collater):\n",
    "            [2, 3, 3, 3, 4, 5, 5, 5, 6, 7, 7, 7, 8, <speechreps>, 1, 1, 1, ...]\n",
    "    \"\"\"\n",
    "    # double check that we are dealing with a seq output by bpe tokenizer\n",
    "    assert graphemes[0] == bpe_whitespace_tok, f\"graphemes == {graphemes}\"\n",
    "    \n",
    "    if boundary_start_pos is None:\n",
    "        boundary_start_pos = padding_idx + 1\n",
    "\n",
    "    if boundary_same_pos:\n",
    "        word_count = boundary_start_pos\n",
    "    else:\n",
    "        word_count = padding_idx\n",
    "        \n",
    "    word_and_word_pos = []\n",
    "    word_pos_of_graphemes = []\n",
    "    current_word = \"\"\n",
    "\n",
    "    for i, c in enumerate(graphemes):\n",
    "        # reached the last symbol of the utt\n",
    "        if c == eos_symbol:\n",
    "            word_and_word_pos.append((current_word, word_count))  # add last word\n",
    "            word_pos_of_graphemes.append(word_count+1)\n",
    "\n",
    "        # whitespace\n",
    "        elif c == bpe_whitespace_tok:\n",
    "            if current_word:  # at a whitespace token AFTER processing at least one word\n",
    "                word_and_word_pos.append((current_word, word_count))\n",
    "                current_word = \"\"\n",
    "            if boundary_same_pos:\n",
    "                word_pos_of_graphemes.append(boundary_start_pos)\n",
    "            else:\n",
    "                word_count += 1  # because we count each whitespace_tok as a new word position\n",
    "                word_pos_of_graphemes.append(word_count)\n",
    "\n",
    "        # processing a grapheme in a word\n",
    "        else:\n",
    "            if graphemes[i - 1] == bpe_whitespace_tok:\n",
    "                word_count += 1  # only increment word position if we are at the beginning of a new word, not within it\n",
    "            word_pos_of_graphemes.append(word_count)\n",
    "            current_word += c\n",
    "\n",
    "    if append_eos:\n",
    "        word_pos_of_graphemes.append(word_count + 1)\n",
    "\n",
    "    return word_and_word_pos, word_pos_of_graphemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5c69703b-1d48-4148-8bf4-91454ee135e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'h', 'o', 'w', '▁', 'a', 'r', 'e', '▁', 'y', 'o', 'u', '</s>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('how', 3), ('are', 4), ('you', 5)], [2, 3, 3, 3, 2, 4, 4, 4, 2, 5, 5, 5, 6])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphemes = '▁ h o w ▁ a r e ▁ y o u </s>'.split(' ')\n",
    "print(graphemes)\n",
    "\n",
    "padding_idx = 1\n",
    "\n",
    "get_word_pos(graphemes, padding_idx=padding_idx, bpe_whitespace_tok=\"▁\", boundary_same_pos=True,\n",
    "                 append_eos=False, eos_symbol = \"</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dd52f466-589a-4773-815d-c8c7063947b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('how', 3), ('are', 5), ('you', 7)], [2, 3, 3, 3, 4, 5, 5, 5, 6, 7, 7, 7, 8])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_pos(graphemes, padding_idx=padding_idx, bpe_whitespace_tok=\"▁\", boundary_same_pos=False,\n",
    "                 append_eos=False, eos_symbol = \"</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b569197e-7b69-452a-8d90-9e18ce837194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should be [2, 3, 3, 3, 4, 5, 5, 5, 6, 7, 7, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "print(\"should be [2, 3, 3, 3, 4, 5, 5, 5, 6, 7, 7, 7, 8]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c1941-0bdb-4e3b-b3e8-00469a890735",
   "metadata": {},
   "source": [
    "# adapt sinusoidal positional embedding to take in positions as an argument rather than just build then one per timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "94190b17-c56c-4f1e-9f74-00c2aa71bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import math\n",
    "from typing import Any, Optional\n",
    "\n",
    "import torch\n",
    "import torch.onnx.operators\n",
    "from fairseq import utils\n",
    "from torch import Tensor, nn\n",
    "\n",
    "\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\n",
    "\n",
    "    Padding symbols are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, padding_idx, init_size=1024):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx if padding_idx is not None else 0\n",
    "        self.weights = SinusoidalPositionalEmbedding.get_embedding(\n",
    "            init_size, embedding_dim, padding_idx\n",
    "        )\n",
    "        self.onnx_trace = False\n",
    "        self.register_buffer(\"_float_tensor\", torch.FloatTensor(1))\n",
    "        self.max_positions = int(1e5)\n",
    "\n",
    "    def prepare_for_onnx_export_(self):\n",
    "        self.onnx_trace = True\n",
    "\n",
    "    @staticmethod\n",
    "    def get_embedding(\n",
    "        num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"Build sinusoidal embeddings.\n",
    "\n",
    "        This matches the implementation in tensor2tensor, but differs slightly\n",
    "        from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "        \"\"\"\n",
    "        half_dim = embedding_dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n",
    "        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(\n",
    "            1\n",
    "        ) * emb.unsqueeze(0)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(\n",
    "            num_embeddings, -1\n",
    "        )\n",
    "        if embedding_dim % 2 == 1:\n",
    "            # zero pad\n",
    "            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n",
    "        if padding_idx is not None:\n",
    "            emb[padding_idx, :] = 0\n",
    "        return emb\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input,\n",
    "        incremental_state: Optional[Any] = None,\n",
    "        timestep: Optional[Tensor] = None,\n",
    "        positions: Optional[Any] = None,\n",
    "    ):\n",
    "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
    "        bspair = torch.onnx.operators.shape_as_tensor(input)\n",
    "        bsz, seq_len = bspair[0], bspair[1]\n",
    "        max_pos = self.padding_idx + 1 + seq_len\n",
    "        if self.weights is None or max_pos > self.weights.size(0):\n",
    "            # recompute/expand embeddings if needed\n",
    "            self.weights = SinusoidalPositionalEmbedding.get_embedding(\n",
    "                max_pos, self.embedding_dim, self.padding_idx\n",
    "            )\n",
    "        self.weights = self.weights.to(self._float_tensor)\n",
    "\n",
    "        if incremental_state is not None:\n",
    "            # positions is the same for every token when decoding a single step\n",
    "            pos = timestep.view(-1)[0] + 1 if timestep is not None else seq_len\n",
    "            if self.onnx_trace:\n",
    "                return (\n",
    "                    self.weights.index_select(index=self.padding_idx + pos, dim=0)\n",
    "                    .unsqueeze(1)\n",
    "                    .repeat(bsz, 1, 1)\n",
    "                )\n",
    "            return self.weights[self.padding_idx + pos, :].expand(bsz, 1, -1)\n",
    "\n",
    "        if positions is None:\n",
    "            positions = utils.make_positions(\n",
    "                input, self.padding_idx, onnx_trace=self.onnx_trace\n",
    "            )\n",
    "\n",
    "        if self.onnx_trace:\n",
    "            flat_embeddings = self.weights.detach().index_select(0, positions.view(-1))\n",
    "            embedding_shape = torch.cat(\n",
    "                (bsz.view(1), seq_len.view(1), torch.tensor([-1], dtype=torch.long))\n",
    "            )\n",
    "            embeddings = torch.onnx.operators.reshape_from_tensor_shape(\n",
    "                flat_embeddings, embedding_shape\n",
    "            )\n",
    "            return embeddings\n",
    "        return (\n",
    "            self.weights.index_select(0, positions.view(-1))\n",
    "            .view(bsz, seq_len, -1)\n",
    "            .detach()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "612e9cfa-a7b7-40ef-8400-63a83f79b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_idx = 1\n",
    "max_source_positions = 1024\n",
    "num_embeddings = max_source_positions\n",
    "pos_emb = SinusoidalPositionalEmbedding(embedding_dim=128, padding_idx=padding_idx, init_size=num_embeddings + padding_idx + 1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4f0d2998-b68e-4d95-bfde-761bd5874a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = torch.Tensor([2, 3, 3, 3, 4, 5, 5, 5, 6, 7, 7, 7, 8, 1, 1, 1]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "430b7757-e35f-4a0a-8b13-6386bf73efde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c09f0e2c-5f4c-4e3d-b368-a1db5499eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce batch dim\n",
    "positions = positions.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8d3adb1a-3d55-4aec-a527-62c8fdf5086a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "27e4c89e-5d5b-48fb-b9db-c4d6f7e31f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 3, 3, 4, 5, 5, 5, 6, 7, 7, 7, 8, 1, 1, 1])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1cdd0972-2a4d-4fba-9ca4-24e4ee2b2456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 3, 3, 4, 5, 5, 5, 6, 7, 7, 7, 8, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "rv = pos_emb(positions, positions=positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "92ddc89e-d735-467f-a121-325f388e6d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9093, 0.9877, 0.9970,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.1411, 0.5224, 0.7847,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.1411, 0.5224, 0.7847,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fa7f0f1d-6c65-437a-98c6-bc84ea8f112c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 128])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "939b0a69-018f-4aa1-9a3e-20e1976a051a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9093,  0.1411,  0.1411,  0.1411, -0.7568, -0.9589, -0.9589, -0.9589,\n",
       "        -0.2794,  0.6570,  0.6570,  0.6570,  0.9894,  0.0000,  0.0000,  0.0000])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a2fa2502-8ee1-4b6f-8df8-af3774932ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9877,  0.5224,  0.5224,  0.5224, -0.3092, -0.9240, -0.9240, -0.9240,\n",
       "        -0.8909, -0.2331, -0.2331, -0.2331,  0.5881,  0.0000,  0.0000,  0.0000])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv[0,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ae56c2b4-72a1-4682-b0b1-64e527d16218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_positions(tensor, padding_idx: int, onnx_trace: bool = False):\n",
    "    \"\"\"Replace non-padding symbols with their position numbers.\n",
    "\n",
    "    Position numbers begin at padding_idx+1. Padding symbols are ignored.\n",
    "    \"\"\"\n",
    "    # The series of casts and type-conversions here are carefully\n",
    "    # balanced to both work with ONNX export and XLA. In particular XLA\n",
    "    # prefers ints, cumsum defaults to output longs, and ONNX doesn't know\n",
    "    # how to handle the dtype kwarg in cumsum.\n",
    "    mask = tensor.ne(padding_idx).int()\n",
    "    return (torch.cumsum(mask, dim=1).type_as(mask) * mask).long() + padding_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ba0ba783-d1ba-4eb2-aec4-4203ae255998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  1,  1,  1]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_positions(positions, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "765a3a5c-c68e-4ce7-8c5c-9bbb935628dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9093, 0.9877, 0.9970,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.9093, 0.9877, 0.9970,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.9093, 0.9877, 0.9970,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        ...,\n",
       "        [0.9093, 0.9877, 0.9970,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.9093, 0.9877, 0.9970,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.9093, 0.9877, 0.9970,  ..., 1.0000, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions = torch.Tensor([2,2,2,2,2,2,2,2,2]).long()\n",
    "positions = positions.unsqueeze(0)\n",
    "pos_emb(positions, positions=positions)[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b12348-395d-4fb9-bc2c-ab38f3d93c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d8ce85-16b0-48f3-bba8-21901d8757ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fairseq] *",
   "language": "python",
   "name": "conda-env-fairseq-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
